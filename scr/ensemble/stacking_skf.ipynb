{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スタッキング\n",
    "・GBDT 2~3個：決定木の深さが「浅い」「普通」「深い」モデル\n",
    "\n",
    "・Random Forest 1~2個：決定木の深さが「浅い」「深い」モデル\n",
    "\n",
    "・Neural Net 1~2個：層の数が「少ない」「多い」モデル\n",
    "\n",
    "・Linier 1個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scr.util import *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scr.models.gbdt import Model1_CatBoost_1, Model1_CatBoost_2, Model1_CatBoost_3, Model1_XGBoost_1, Model1_XGBoost_2, Model1_XGBoost_3, Model1_LightGBM_1, Model1_LightGBM_2, Model1_LightGBM_3\n",
    "from scr.models.random_forest import Model1_RandomForest_1, Model1_RandomForest_2, Model1_RandomForest_3\n",
    "from scr.models.nn import Model1_NN_1, Model1_NN_2, Model1_NN_all_1, Model1_NN_all_2, Model1_TabNet_1, Model1_TabNet_2\n",
    "from scr.models.linear import Model1_Logistic_1, Model1_Logistic_2, Model1_Logistic_3, Model1_Logistic_4, Model2_Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_gb = pd.read_csv('data/sampling/over_sampling/smote/train_mean_gb_smote.csv')\n",
    "df_test_gb = pd.read_csv('data/sampling/over_sampling/smote/test_mean_gb.csv')\n",
    "combination_gb= df_train_gb.columns[44:110]\n",
    "\n",
    "df_train_nn = pd.read_csv('data/sampling/over_sampling/smote/train_mean_nn_smote.csv')\n",
    "df_test_nn = pd.read_csv('data/feature_engineered/null_representative/test_null_mean_scaled.csv')\n",
    "\n",
    "y = df_train_gb['ProdTaken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContractRate_G1    3\n",
      "ContractRate_G2    8\n",
      "ContractRate_G3    6\n",
      "ContractRate_G5    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df_test_gb.isnull().sum()\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量が多いので、落とす特徴量を選択\n",
    "drop_gb = [\n",
    "    'EconomicSegment',\n",
    "    'ContractRate_FM',\n",
    "    'ContractRate_G1',\n",
    "    'ContractRate_G2',\n",
    "    'ContractRate_G3',\n",
    "    'ContractRate_G4',\n",
    "    'ContractRate_G5',\n",
    "    'ContractRate_G6'\n",
    "]\n",
    "\n",
    "# ## --------------------------------------------------------------------------------------------\n",
    "# ## Label Encoding\n",
    "# df_train_gb = mapping_columns_if_exist(df_train_gb)\n",
    "# df_test_gb = mapping_columns_if_exist(df_test_gb)\n",
    "\n",
    "# def handle_unknown_label(train_series, test_series):\n",
    "#     unique_labels = train_series.unique()\n",
    "#     label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "#     train_encoded = train_series.map(label_map)\n",
    "#     test_encoded = test_series.map(lambda x: label_map.get(x, -1))\n",
    "#     return train_encoded, test_encoded\n",
    "\n",
    "# for col in combination_gb:\n",
    "#     df_train_gb[col], df_test_gb[col] = handle_unknown_label(df_train_gb[col], df_test_gb[col])\n",
    "# ## --------------------------------------------------------------------------------------------\n",
    "\n",
    "X_gb = df_train_gb.drop(columns=drop_gb, axis=1)\n",
    "y_gb = df_train_gb['ProdTaken']\n",
    "\n",
    "test_feature = X_gb.columns.drop('ProdTaken')\n",
    "df_test_gb = df_test_gb[test_feature]\n",
    "\n",
    "tmp = X_gb.groupby(by=['AgeGroup', 'ProductPitched'], as_index=False)['ProdTaken'].mean()\n",
    "tmp = tmp.rename(columns={'ProdTaken': 'ContractRate_G4'})\n",
    "df_test_gb = df_test_gb.merge(tmp, on=['AgeGroup', 'ProductPitched'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_nn = [\n",
    "    'EconomicSegment',\n",
    "    'ContractRate_FM',\n",
    "    'ContractRate_G1',\n",
    "    'ContractRate_G2',\n",
    "    'ContractRate_G3',\n",
    "    'ContractRate_G4',\n",
    "    'ContractRate_G5',\n",
    "    'ContractRate_G6'\n",
    "]\n",
    "\n",
    "X_nn = df_train_nn.drop(columns=drop_nn, axis=1)\n",
    "y_nn = df_train_nn['ProdTaken']\n",
    "\n",
    "df_test_nn = df_test_nn.drop(columns=drop_nn, axis=1)\n",
    "\n",
    "num_features_nn = len(df_test_nn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スタッキング\n",
    "def predict_cv(model, X, y, df_test):\n",
    "    preds = []\n",
    "    preds_test = []\n",
    "    va_idxes = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        tr_x, va_x = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        tr_y, va_y = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        \n",
    "        # Target Encoding\n",
    "        if 'AgeGroup' in tr_x.columns:\n",
    "            tmp = tr_x.groupby(by=['AgeGroup', 'ProductPitched'], as_index=False)['ProdTaken'].mean()\n",
    "            tmp = tmp.rename(columns={'ProdTaken': 'ContractRate_G4'})\n",
    "            tr_x = tr_x.merge(tmp, on=['AgeGroup', 'ProductPitched'], how='left')\n",
    "            va_x = va_x.merge(tmp, on=['AgeGroup', 'ProductPitched'], how='left')\n",
    "            \n",
    "            tr_x = tr_x.drop(labels='ProdTaken', axis=1)\n",
    "            va_x = va_x.drop(labels='ProdTaken', axis=1)\n",
    "        \n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        pred_test = model.predict(df_test)\n",
    "        preds_test.append(pred_test)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    # テストデータに対する予測値の平均をとる\n",
    "    preds_test = np.mean(preds_test, axis=0)\n",
    "\n",
    "    return pred_train, preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1層目のモデル\n",
    "models_gbdt = [\n",
    "    Model1_CatBoost_1(),\n",
    "    Model1_CatBoost_2(),\n",
    "    Model1_CatBoost_3(),\n",
    "    #Model1_XGBoost_1(),\n",
    "    #Model1_XGBoost_2(),\n",
    "    #Model1_XGBoost_3(),\n",
    "    #Model1_LightGBM_1(),\n",
    "    #Model1_LightGBM_2(),\n",
    "    #Model1_LightGBM_3(),\n",
    "    #Model1_RandomForest_1(),\n",
    "    Model1_RandomForest_2(),\n",
    "    #Model1_RandomForest_3()\n",
    "]\n",
    "\n",
    "models_nn = [\n",
    "    Model1_NN_1(input_shape=num_features_nn),\n",
    "    Model1_NN_2(input_shape=num_features_nn),\n",
    "    Model1_NN_all_1(input_shape=num_features_nn),\n",
    "    Model1_NN_all_2(input_shape=num_features_nn),\n",
    "    #Model1_TabNet_1(input_dim=num_features_nn),\n",
    "    #Model1_TabNet_2(input_dim=num_features_nn),\n",
    "    Model1_Logistic_1(),\n",
    "    Model1_Logistic_2(),\n",
    "    #Model1_Logistic_3(),\n",
    "    #Model1_Logistic_4()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.8094036\tbest: 0.8094036 (0)\ttotal: 133ms\tremaining: 798ms\n",
      "6:\ttest: 0.8557267\tbest: 0.8557267 (6)\ttotal: 162ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.855726657\n",
      "bestIteration = 6\n",
      "\n",
      "0:\ttest: 0.8214192\tbest: 0.8214192 (0)\ttotal: 5.55ms\tremaining: 33.3ms\n",
      "6:\ttest: 0.8939034\tbest: 0.8939034 (6)\ttotal: 37.9ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.89390344\n",
      "bestIteration = 6\n",
      "\n",
      "0:\ttest: 0.8252564\tbest: 0.8252564 (0)\ttotal: 5.9ms\tremaining: 35.4ms\n",
      "6:\ttest: 0.8999377\tbest: 0.9024531 (4)\ttotal: 34.3ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9024530851\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8308399\tbest: 0.8308399 (0)\ttotal: 2.94ms\tremaining: 17.6ms\n",
      "6:\ttest: 0.9103816\tbest: 0.9120748 (5)\ttotal: 20ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9120747511\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n",
      "0:\ttest: 0.8123161\tbest: 0.8123161 (0)\ttotal: 4.78ms\tremaining: 28.7ms\n",
      "6:\ttest: 0.8924341\tbest: 0.8929318 (4)\ttotal: 29.7ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8929318464\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8466312\tbest: 0.8466312 (0)\ttotal: 5.1ms\tremaining: 30.6ms\n",
      "6:\ttest: 0.8964886\tbest: 0.8964886 (6)\ttotal: 32.6ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8964885735\n",
      "bestIteration = 6\n",
      "\n",
      "0:\ttest: 0.8477270\tbest: 0.8477270 (0)\ttotal: 8.66ms\tremaining: 52ms\n",
      "6:\ttest: 0.9249432\tbest: 0.9249432 (6)\ttotal: 37.8ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9249431885\n",
      "bestIteration = 6\n",
      "\n",
      "0:\ttest: 0.8638548\tbest: 0.8638548 (0)\ttotal: 11.9ms\tremaining: 71.2ms\n",
      "6:\ttest: 0.9204234\tbest: 0.9213712 (4)\ttotal: 50.1ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9213711816\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8735923\tbest: 0.8735923 (0)\ttotal: 6.01ms\tremaining: 36ms\n",
      "6:\ttest: 0.9232305\tbest: 0.9241671 (4)\ttotal: 32.6ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9241670901\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8612404\tbest: 0.8612404 (0)\ttotal: 5.29ms\tremaining: 31.8ms\n",
      "6:\ttest: 0.9142655\tbest: 0.9177051 (4)\ttotal: 33.6ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9177050592\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8586203\tbest: 0.8586203 (0)\ttotal: 8.69ms\tremaining: 52.1ms\n",
      "6:\ttest: 0.9131859\tbest: 0.9146794 (5)\ttotal: 54.1ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9146794267\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n",
      "0:\ttest: 0.8742427\tbest: 0.8742427 (0)\ttotal: 17.6ms\tremaining: 105ms\n",
      "6:\ttest: 0.9354652\tbest: 0.9389339 (5)\ttotal: 61.6ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9389338976\n",
      "bestIteration = 5\n",
      "\n",
      "Shrink model to first 6 iterations.\n",
      "0:\ttest: 0.8766981\tbest: 0.8766981 (0)\ttotal: 12.3ms\tremaining: 74.1ms\n",
      "6:\ttest: 0.9359858\tbest: 0.9359858 (6)\ttotal: 55ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9359858404\n",
      "bestIteration = 6\n",
      "\n",
      "0:\ttest: 0.8839649\tbest: 0.8839649 (0)\ttotal: 12.2ms\tremaining: 73.3ms\n",
      "6:\ttest: 0.9367326\tbest: 0.9387147 (4)\ttotal: 56.5ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9387147475\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n",
      "0:\ttest: 0.8959478\tbest: 0.8959478 (0)\ttotal: 12.5ms\tremaining: 75.2ms\n",
      "6:\ttest: 0.9330782\tbest: 0.9350678 (4)\ttotal: 61.3ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9350678404\n",
      "bestIteration = 4\n",
      "\n",
      "Shrink model to first 5 iterations.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m pred_test_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models_gbdt:\n\u001b[1;32m----> 4\u001b[0m     pred_train, pred_test \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_gb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_gb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     pred_train_list\u001b[38;5;241m.\u001b[39mappend(pred_train)\n\u001b[0;32m      6\u001b[0m     pred_test_list\u001b[38;5;241m.\u001b[39mappend(pred_test)\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36mpredict_cv\u001b[1;34m(model, X, y, df_test)\u001b[0m\n\u001b[0;32m     21\u001b[0m     va_x \u001b[38;5;241m=\u001b[39m va_x\u001b[38;5;241m.\u001b[39mdrop(labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProdTaken\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(tr_x, tr_y, va_x, va_y)\n\u001b[1;32m---> 24\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mva_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(pred)\n\u001b[0;32m     26\u001b[0m pred_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(df_test)\n",
      "File \u001b[1;32mc:\\Users\\ricke\\Python\\Competition\\SIGNATE\\SIGNATE Summer Cup 2024\\scr\\models\\random_forest.py:47\u001b[0m, in \u001b[0;36mModel1_RandomForest_2.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ricke\\anaconda3\\envs\\neuralnet\\lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "pred_train_list = []\n",
    "pred_test_list = []\n",
    "for model in models_gbdt:\n",
    "    pred_train, pred_test = predict_cv(model, X_gb, y_gb, df_test_gb)\n",
    "    pred_train_list.append(pred_train)\n",
    "    pred_test_list.append(pred_test)\n",
    "for model in models_nn:\n",
    "    pred_train, pred_test = predict_cv(model, X_nn, y_nn, df_test_nn)\n",
    "    pred_train_list.append(pred_train)\n",
    "    pred_test_list.append(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for model 1: 0.8087502286445948\n",
      "AUC for model 2: 0.8259819612864352\n",
      "AUC for model 3: 0.8290901828080783\n",
      "AUC for model 4: 0.8206585771312366\n",
      "AUC for model 5: 0.8435089144492625\n",
      "AUC for model 6: 0.8460125727627799\n",
      "AUC for model 7: 0.910081478173856\n",
      "AUC for model 8: 0.915961006681802\n",
      "AUC for model 9: 0.8305629229925003\n",
      "AUC for model 10: 0.8260270177213009\n"
     ]
    }
   ],
   "source": [
    "# 1層目のモデルの評価\n",
    "for i, pred_train in enumerate(pred_train_list):\n",
    "    auc_score = roc_auc_score(y, pred_train)\n",
    "    print(f'AUC for model {i+1}: {auc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量として使用する列名を作成\n",
    "column_names = [f'pred_{i+1}' for i in range(len(pred_train_list))]\n",
    "\n",
    "# 予測値を特徴量としてデータフレームを作成\n",
    "train_x_2 = pd.DataFrame(\n",
    "    {f'pred_{i+1}': pred_train_list[i] for i in range(len(pred_train_list))},\n",
    "    columns=column_names\n",
    ")\n",
    "\n",
    "test_x_2 = pd.DataFrame(\n",
    "    {f'pred_{i+1}': pred_test_list[i] for i in range(len(pred_test_list))},\n",
    "    columns=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9141412647005026\n"
     ]
    }
   ],
   "source": [
    "# 2層目のモデル\n",
    "# pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n",
    "# pred_test_2は、2層目のモデルのテストデータの予測値\n",
    "model_2 = Model2_Logistic()\n",
    "pred_train_2, pred_test_2 = predict_cv(model_2, train_x_2, y, test_x_2)\n",
    "print(f'AUC: {roc_auc_score(y, pred_train_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv('data/test.csv')['id'].values\n",
    "df_submit = pd.DataFrame({\n",
    "    \"id\": index,\n",
    "    \"prediction\": pred_test_2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'submission/submit_27.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.to_csv(path, index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
